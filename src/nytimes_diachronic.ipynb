{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from evolvemb import load_diachronic_dataset, compute_emb_snapshots, most_changed_tokens, analyze_emb_over_time\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emb_snapshots(snapshots, start_date=\"2019-01-01\", local_emb_name=\"dummy\", min_freq=100, n_tokens=10000, \n",
    "                      saveemb=False, datapath=\"data/nytimes_dataset.txt\"):\n",
    "    if local_emb_name.startswith(\"data/\"):\n",
    "        # e.g. for a fine-tuned model saved in the data folder\n",
    "        savepath = f\"data/snapshot_emb_{local_emb_name.lower()[5:]}_{start_date}_{snapshots[-1]}_{min_freq}.pkl\"\n",
    "    else:\n",
    "        savepath = f\"data/snapshot_emb_{local_emb_name.lower()}_{start_date}_{snapshots[-1]}_{min_freq}.pkl\"\n",
    "    # see if we can just load the embeddings\n",
    "    if os.path.exists(savepath):\n",
    "        try:\n",
    "            snapshot_emb = pickle.load(open(savepath, \"rb\"))\n",
    "            return snapshot_emb\n",
    "        except Exception as e:\n",
    "            print(\"could not load embeddings:\", e)\n",
    "    # load dataset\n",
    "    sentences, dates = load_diachronic_dataset(datapath, start_date, snapshots[-1])\n",
    "    # compute snapshots\n",
    "    snapshot_emb = compute_emb_snapshots(sentences, dates, snapshots, local_emb_name, min_freq, n_tokens)\n",
    "    # possibly save embeddings\n",
    "    if saveemb:\n",
    "        try:\n",
    "            pickle.dump(snapshot_emb, open(savepath, \"wb\"), -1)\n",
    "            print(f\"successfully saved embeddings at {savepath}\")\n",
    "        except Exception as e:\n",
    "            print(\"error saving embeddings:\", e)\n",
    "    return snapshot_emb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimal Example (to create embedding snapshots for the Dash App)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# desired snapshot dates: pre- and post-corona outbreak in detail\n",
    "snapshots = [f\"2019-{i:02}-31\" for i in range(6, 13)] + [f\"2020-{i:02}-31\" for i in range(1, 13)]\n",
    "# compute embedding snapshots with \"bert-base-uncased\" (can be abbreviated as \"bert\"; only works for bert and roberta)\n",
    "snapshot_emb = get_emb_snapshots(snapshots, start_date=\"2019-04-01\", local_emb_name=\"bert\", min_freq=50)\n",
    "# save embeddings to use with app.py\n",
    "pickle.dump(snapshot_emb, open(\"snapshot_emb.pkl\", \"wb\"), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see which words have changed the most at some point in the time period\n",
    "most_changed = most_changed_tokens(snapshot_emb, ignore_zeros=True)\n",
    "print(\"most changed tokens (ignoring new words)\")\n",
    "print(\"\\n\".join([f\"{x[0]:15} ({x[1]:.4f})\" for x in most_changed[:25]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create interactive plots for word \"category\"\n",
    "fig_time, fig_pca = analyze_emb_over_time(snapshot_emb, \"category\")\n",
    "fig_time.show()\n",
    "fig_pca.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Analysis (to reproduce results from paper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare different transformer architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_cosine_sim_knn(snapshot_emb, k=10):\n",
    "    # check the overlap between cosine similarity and knn intersection score (Gonen et al., 2020)\n",
    "    snapshots = sorted(snapshot_emb)\n",
    "    f, l = snapshots[0], snapshots[-1]\n",
    "    token_sim = []\n",
    "    token_knn_score = []\n",
    "    # ignore words that had a zero embedding in the beginning\n",
    "    tokens = [t for t in snapshot_emb[f].input_model.index2token if np.any(snapshot_emb[f][t] != 0)]\n",
    "    for i, t in enumerate(tokens):\n",
    "        if not i%100: print(f\"Processing {i+1:6}/{len(tokens)}\", end=\"\\r\")\n",
    "        token_sim.append(cosine_similarity(snapshot_emb[f][t][None, :], snapshot_emb[l][t][None, :])[0, 0])\n",
    "        knn1 = set(snapshot_emb[f].get_nneighbors(t, k, include_simscore=False))\n",
    "        knn2 = set(snapshot_emb[l].get_nneighbors(t, k, include_simscore=False))\n",
    "        token_knn_score.append(len(knn1.intersection(knn2))/k)\n",
    "    print(f\"Processing {len(tokens):6}/{len(tokens)}\")\n",
    "    token_sim, token_knn_score = np.array(token_sim), np.array(token_knn_score)\n",
    "    plt.figure()\n",
    "    plt.scatter(token_sim, token_knn_score)\n",
    "    plt.xlabel(\"cosine similarity\")\n",
    "    plt.ylabel(f\"intersection of NN @ k={k}\")\n",
    "    plt.title(f\"correlation: {pearsonr(token_sim, token_knn_score)[0]:.3f}\")\n",
    "    return tokens, token_sim, token_knn_score\n",
    "\n",
    "\n",
    "def compare_most_changed_tokens(tokens1, tokens2, name1, name2, c=\"#7C0033\", new_fig=True):\n",
    "    # compare the similarity scores of the most changed tokens from two models\n",
    "    tokens1, tokens2 = dict(tokens1), dict(tokens2)\n",
    "    tokens = set(tokens1.keys())\n",
    "    tokens.intersection_update(tokens2.keys())\n",
    "    tokens = sorted(tokens)\n",
    "    scores1 = np.array([tokens1[t] for t in tokens])\n",
    "    scores2 = np.array([tokens2[t] for t in tokens])\n",
    "    if new_fig:\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        plt.grid()\n",
    "    plt.scatter(scores1, scores2, s=10, c=c, alpha=0.5)\n",
    "    plt.xlabel(name1, fontsize=14)\n",
    "    plt.ylabel(name2, fontsize=14)\n",
    "    corr = pearsonr(scores1, scores2)[0]\n",
    "    plt.title(f\"correlation: {corr:.3f}\")\n",
    "    return corr\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# desired snapshot dates: pre- and post-corona outbreak in detail\n",
    "snapshots = [f\"2019-{i:02}-31\" for i in range(6, 13)] + [f\"2020-{i:02}-31\" for i in range(1, 13)]\n",
    "\n",
    "def run_analysis(local_emb_name=\"dummy\", savefigs=\"\", check_knn_score=False):   \n",
    "    # generate/load embeddings\n",
    "    snapshot_emb = get_emb_snapshots(snapshots, start_date=\"2019-04-01\", local_emb_name=local_emb_name, min_freq=50, n_tokens=10000, saveemb=True)\n",
    "    # see which words have changed the most at some point in the time period\n",
    "    most_changed = most_changed_tokens(snapshot_emb, ignore_zeros=True)\n",
    "    print(\"most changed tokens (ignore_zeros=True)\")\n",
    "    print(\"\\n\".join([f\"{x[0]:15} ({x[1]:.4f})\" for x in most_changed[:25]]))\n",
    "    # see which words are new\n",
    "    print(\"most changed tokens (ignore_zeros=False)\")\n",
    "    print(\"\\n\".join([f\"{x[0]:15} ({x[1]:.4f})\" for x in most_changed_tokens(snapshot_emb, ignore_zeros=False)[:25]]))\n",
    "    if check_knn_score:\n",
    "        # see in how far the cosine similarity and knn intersection score agree\n",
    "        for k in [10, 100, 1000]:\n",
    "            tokens, token_sim, token_knn_score = test_cosine_sim_knn(snapshot_emb, k=k)\n",
    "    # create plots from paper\n",
    "    fig_time, fig_pca = analyze_emb_over_time(snapshot_emb, \"positive\", k=5, savefigs=savefigs)\n",
    "    fig_time.show()\n",
    "    fig_pca.show()\n",
    "    fig_time, fig_pca = analyze_emb_over_time(snapshot_emb, \"category\", k=5, savefigs=savefigs)\n",
    "    fig_time.show()\n",
    "    fig_pca.show()\n",
    "    return snapshot_emb, most_changed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# run analysis for bert\n",
    "snapshot_emb, bert_most_changed = run_analysis(local_emb_name=\"bert\", savefigs=\"bert\")\n",
    "_ = analyze_emb_over_time(snapshot_emb, \"biden\", k=5, savefigs=\"bert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# same analysis for roberta\n",
    "_, roberta_most_changed = run_analysis(local_emb_name=\"roberta\")\n",
    "# and both finetuned models\n",
    "snapshot_emb, bert_ft_most_changed = run_analysis(local_emb_name=\"data/nyt_bert\")\n",
    "_ = analyze_emb_over_time(snapshot_emb, \"biden\", k=5, savefigs=\"nyt_bert\")\n",
    "_, roberta_ft_most_changed = run_analysis(local_emb_name=\"data/nyt_roberta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see in how far the most changed tokens from BERT and RoBERTa agree (before and after fine-tuning)\n",
    "_ = compare_most_changed_tokens(bert_most_changed, bert_ft_most_changed, \"BERT\", \"BERT (fine-tuned)\")\n",
    "_ = compare_most_changed_tokens(roberta_most_changed, roberta_ft_most_changed, \"RoBERTa\", \"RoBERTa (fine-tuned)\")\n",
    "corr1 = compare_most_changed_tokens(bert_most_changed, roberta_most_changed, \"BERT\", \"RoBERTa\")\n",
    "corr_ft = compare_most_changed_tokens(bert_ft_most_changed, roberta_ft_most_changed, \"BERT\", \"RoBERTa\", c=\"#00537C\", new_fig=False)\n",
    "plt.title(\"\")\n",
    "plt.legend([f\"pre-trained $(r: {corr1:.3f})$\", f\"fine-tuned $(r: {corr_ft:.3f})$\"], fontsize=14)\n",
    "plt.savefig(\"diachr_score_agreement.pdf\", dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate on data with artificial semantic (non-)shifts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check most changed tokens when sentences are shuffled \n",
    "# (i.e. determine threshold on cosine similarity to avoid false positives)\n",
    "savepath = f\"data/snapshot_emb_shuffled_2019-04-01_{snapshots[-1]}_50.pkl\"\n",
    "# see if we can just load the embeddings\n",
    "if os.path.exists(savepath):\n",
    "    snapshot_emb = pickle.load(open(savepath, \"rb\"))\n",
    "else:\n",
    "    # load dataset\n",
    "    sentences, dates = load_diachronic_dataset(\"data/nytimes_dataset.txt\", \"2019-04-01\", snapshots[-1])\n",
    "    # shuffle sentences (but leave dates as they were!)\n",
    "    random.seed(10)\n",
    "    random.shuffle(sentences)  # inplace\n",
    "    # compute snapshots as before with shuffled sentences\n",
    "    snapshot_emb = compute_emb_snapshots(sentences, dates, snapshots, \"bert\", 50)\n",
    "    pickle.dump(snapshot_emb, open(savepath, \"wb\"), -1)\n",
    "# check which are now the most changed words\n",
    "most_changed = most_changed_tokens(snapshot_emb, ignore_zeros=True)\n",
    "print(\"most changed tokens (ignore_zeros=True)\")\n",
    "print(\"\\n\".join([f\"{x[0]:15} ({x[1]:.4f})\" for x in most_changed[:25]]))\n",
    "# example plot for our previous most changed token\n",
    "fig_time, fig_pca = analyze_emb_over_time(snapshot_emb, \"category\", k=5, savefigs=\"shuffled\")\n",
    "fig_time.show()\n",
    "fig_pca.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load original bert checkpoint\n",
    "savepath = f\"data/snapshot_emb_bert_2019-04-01_{snapshots[-1]}_50.pkl\"\n",
    "snapshot_emb = pickle.load(open(savepath, \"rb\"))\n",
    "# select two words that occur fairly often and that don't have too much in common\n",
    "# the input model of the embeddings already contains counts of the tokens, check the 100 most frequent\n",
    "print(snapshot_emb[snapshots[-1]].input_model.token_counts.most_common(100))\n",
    "# select two words from which we believe they aren't too similar\n",
    "word1 = \"president\"\n",
    "word2 = \"coronavirus\"\n",
    "# check their cosine similarities to be sure they really are not very similar\n",
    "print(f\"cosine similarity between {word1} and {word2}\", cosine_similarity(snapshot_emb[snapshots[-1]][word1][None, :], snapshot_emb[snapshots[-1]][word2][None, :]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at plots for both words to check their original nearest neighbors over time\n",
    "fig_time, _ = analyze_emb_over_time(snapshot_emb, word1)\n",
    "fig_time.show()\n",
    "fig_time, _ = analyze_emb_over_time(snapshot_emb, word2)\n",
    "fig_time.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "sentences, dates = load_diachronic_dataset(\"data/nytimes_dataset.txt\", \"2019-04-01\", snapshots[-1])\n",
    "# split the original list with sentences into 3 list: those with word1, with word2, and without any of the words\n",
    "sentences_word1 = []\n",
    "sentences_word2 = []\n",
    "sentences_without = []\n",
    "dates_without = []\n",
    "# create an artificial new word as a combination of both words\n",
    "newword = f\"{word1}{word2}\"\n",
    "for i, s in enumerate(sentences):\n",
    "    if word1 in s:\n",
    "        # ignore sentences with both words\n",
    "        if word2 in s:\n",
    "            continue\n",
    "        # replace original word with artificial word\n",
    "        sentences_word1.append([newword if w == word1 else w for w in s])\n",
    "    elif word2 in s:\n",
    "        sentences_word2.append([newword if w == word2 else w for w in s])\n",
    "    else:\n",
    "        sentences_without.append(s)\n",
    "        dates_without.append(dates[i])\n",
    "print(f\"number of sentences with {word1}:\", len(sentences_word1))\n",
    "print(f\"number of sentences with {word2}:\", len(sentences_word2))\n",
    "print(\"number of sentences without the words:\", len(sentences_without))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigmoid function based on which we'll draw the sentences\n",
    "def sigm(i, n):\n",
    "    return 1/(1+np.exp(-(i-n/2)/(n/10)))\n",
    "\n",
    "# check that it looks correctly independent of the number of sentences\n",
    "# for n in [1000, 10000]:\n",
    "#     x = np.arange(n)\n",
    "#     plt.figure()\n",
    "#     plt.plot(x, sigm(x, n));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle both sets of sentences and take the same number from each\n",
    "random.seed(23)\n",
    "random.shuffle(sentences_word1)\n",
    "random.shuffle(sentences_word2)\n",
    "min_len = min(len(sentences_word1), len(sentences_word2))\n",
    "sentences_word1, sentences_word2 = sentences_word1[:min_len], sentences_word2[:min_len]\n",
    "# combine both lists into a single list where we first have a high priority of choosing sentences from \n",
    "# the first word and then from the second\n",
    "sentences_both = []\n",
    "n = len(sentences_word1)+len(sentences_word2)\n",
    "for i in range(n):\n",
    "    # add either a sentence with word1 or word2 depending on sigmoid threshold\n",
    "    if (len(sentences_word1) > len(sentences_word2)) or (len(sentences_word1) and random.random() >= sigm(i, n)):\n",
    "        sentences_both.append(sentences_word1.pop())\n",
    "    else:\n",
    "        sentences_both.append(sentences_word2.pop())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check some sentences at the beginning ... all about word1\n",
    "print(\"\\n\".join([\" \".join(s) for s in sentences_both[:10]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... and some at the end; they are about word2\n",
    "print(\"\\n\".join([\" \".join(s) for s in sentences_both[-10:]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interleave the new sentences with the originals\n",
    "sentences_new = []\n",
    "dates_new = []\n",
    "# every r_th sentence should be from our artificial list\n",
    "r = len(sentences_without) // len(sentences_both)\n",
    "n = len(sentences_without)\n",
    "i_both = 0\n",
    "for i in range(n):\n",
    "    # always add the original sentence\n",
    "    sentences_new.append(sentences_without[i])\n",
    "    dates_new.append(dates_without[i])\n",
    "    # in between add a sentence for the new list\n",
    "    if not i % r and i_both < len(sentences_both):\n",
    "        sentences_new.append(sentences_both[i_both])\n",
    "        i_both += 1\n",
    "        # add the same date again\n",
    "        dates_new.append(dates_without[i])\n",
    "# possibly add a last new sentence\n",
    "if i_both < len(sentences_both):\n",
    "    sentences_new.append(sentences_both[i_both])\n",
    "    dates_new.append(dates_without[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save new sentences as a dataset to fine tune bert on\n",
    "with open(f\"data/nytimes_dataset_{newword}.txt\", \"w\") as f:\n",
    "    f.write(\"\\n\".join([f\"{dates_new[i]}\\t{' '.join(sentences_new[i])}\" for i in range(len(dates_new))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute snapshots from our new sentences\n",
    "savepath = f\"data/snapshot_emb_bert_{newword}_2019-04-01_{snapshots[-1]}_50.pkl\"\n",
    "# see if we can just load the embeddings\n",
    "if os.path.exists(savepath):\n",
    "    snapshot_emb = pickle.load(open(savepath, \"rb\"))\n",
    "else:\n",
    "    snapshot_emb = compute_emb_snapshots(sentences_new, dates_new, snapshots, \"bert\", 50)\n",
    "    pickle.dump(snapshot_emb, open(savepath, \"wb\"), -1)\n",
    "# check which are now the most changed words\n",
    "most_changed = most_changed_tokens(snapshot_emb, ignore_zeros=True)\n",
    "print(\"most changed tokens (ignore_zeros=True)\")\n",
    "print(\"\\n\".join([f\"{x[0]:15} ({x[1]:.4f})\" for x in most_changed[:25]]))\n",
    "# example plot for our new word\n",
    "fig_time, fig_pca = analyze_emb_over_time(snapshot_emb, newword, k=5, savefigs=\"bert\")\n",
    "fig_time.show()\n",
    "fig_pca.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute snapshots from our new sentences with the fine-tuned model\n",
    "savepath = f\"data/snapshot_emb_nyt_bert_{newword}_2019-04-01_{snapshots[-1]}_50.pkl\"\n",
    "# see if we can just load the embeddings\n",
    "if os.path.exists(savepath):\n",
    "    snapshot_emb = pickle.load(open(savepath, \"rb\"))\n",
    "else:\n",
    "    snapshot_emb = compute_emb_snapshots(sentences_new, dates_new, snapshots, f\"data/nyt_bert_{newword}\", 50)\n",
    "    pickle.dump(snapshot_emb, open(savepath, \"wb\"), -1)\n",
    "# check which are now the most changed words\n",
    "most_changed = most_changed_tokens(snapshot_emb, ignore_zeros=True)\n",
    "print(\"most changed tokens (ignore_zeros=True)\")\n",
    "print(\"\\n\".join([f\"{x[0]:15} ({x[1]:.4f})\" for x in most_changed[:25]]))\n",
    "# example plot for our new word\n",
    "fig_time, fig_pca = analyze_emb_over_time(snapshot_emb, newword, k=5, savefigs=\"nyt_bert\")\n",
    "fig_time.show()\n",
    "fig_pca.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
