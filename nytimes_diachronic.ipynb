{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import KernelPCA\n",
    "import plotly.express as px\n",
    "from flair.embeddings import TransformerWordEmbeddings\n",
    "\n",
    "from evolvemb import EvolvingEmbeddings, DummyEmbeddings\n",
    "from emb_noflair import SimplePretrainedEmbeddings\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_nyt(start_date=\"2019-01-01\", end_date=\"2020-12-31\"):\n",
    "    # read in NYT dataset\n",
    "    sentences = []\n",
    "    dates = []\n",
    "    with open(\"data/nytimes_dataset.txt\") as f:\n",
    "        for line in f:\n",
    "            d, s = line.strip().split(\"\\t\")\n",
    "            if d < start_date:\n",
    "                continue\n",
    "            elif d > end_date:\n",
    "                break\n",
    "            dates.append(d)\n",
    "            # lowercase! and some longer words mistakenly can end with \".\" due to the tokenizer; remove this!\n",
    "            sentences.append([w if len(w) <= 3 or not w.endswith(\".\") else w[:-1] for w in s.lower().split()])\n",
    "    print(\"Dataset contains %i sentences between %s and %s\" % (len(sentences), start_date, end_date))\n",
    "    return sentences, dates\n",
    "\n",
    "\n",
    "def get_emb_snapshots(snapshots, start_date=\"2019-01-01\", local_emb_name=\"dummy\", min_freq=100, n_tokens=10000, saveemb=False):\n",
    "    savepath = \"data/snapshot_emb_%s_%s_%s_%i.pkl\" % (local_emb_name.lower(), start_date, snapshots[-1], min_freq)\n",
    "    # see if we can just load the embeddings\n",
    "    if os.path.exists(savepath):\n",
    "        try:\n",
    "            snapshot_emb = pickle.load(open(savepath, \"rb\"))\n",
    "            return snapshot_emb\n",
    "        except:\n",
    "            pass\n",
    "    # load dataset\n",
    "    sentences, dates = load_nyt(start_date, snapshots[-1])\n",
    "    # transformer model to generate the local embeddings\n",
    "    if local_emb_name.lower() == \"dummy\":\n",
    "        local_emb = DummyEmbeddings(50, \"testemb\")  # for some quick testing only\n",
    "    elif local_emb_name.lower() == \"bert\":\n",
    "        local_emb = TransformerWordEmbeddings(\"bert-base-uncased\", layers=\"all\", use_scalar_mix=True, pooling_operation=\"mean\", fine_tune=False)\n",
    "    elif local_emb_name.lower() == \"roberta\":\n",
    "        local_emb = TransformerWordEmbeddings(\"roberta-base\", layers=\"all\", use_scalar_mix=True, pooling_operation=\"mean\", fine_tune=False)\n",
    "    else:\n",
    "        local_emb = TransformerWordEmbeddings(os.path.join(\"data\", local_emb_name), layers=\"all\", use_scalar_mix=True, pooling_operation=\"mean\", fine_tune=False)\n",
    "    # pass sentences directly to generate input model from all texts \n",
    "    # so we know which words are of interest and their count (to set alphas manually afterwards)\n",
    "    emb = EvolvingEmbeddings(local_emb, sentences, alpha=None, min_freq=min_freq, n_tokens=n_tokens, update_index=False)\n",
    "    print(\"Number of words in the vocabulary (for which we'll learn an embedding):\", emb.input_model.n_tokens)\n",
    "    # create counts dict based on token_counts/interval\n",
    "    counts_dict = {t: emb.input_model.token_counts[t]/(1.*len(snapshots)) for t in emb.input_model.index2token}\n",
    "    # manually create max_counts array with entries for individual words\n",
    "    emb._set_max_count(counts_dict)\n",
    "    # compute evolving embeddings and take snapshot at the end of each split\n",
    "    snapshot_emb = {}\n",
    "    current_snap = 0\n",
    "    for i, s in enumerate(sentences):\n",
    "        if not i % 100:\n",
    "            print(\"Processing sentence %8i/%i\" % (i, len(sentences)), end=\"\\r\")\n",
    "        # check if we need to take the snapshot\n",
    "        if dates[i] > snapshots[current_snap]:\n",
    "            actual_snap = dates[i-1]  # the given snapshot might be at the 31st but the month could have only 30 days\n",
    "            temp = emb.as_pretrained()\n",
    "            # save as a model without flair dependency\n",
    "            snapshot_emb[actual_snap] = SimplePretrainedEmbeddings(temp.embeddings, temp.input_model)\n",
    "            # set OOV embedding to zeros\n",
    "            snapshot_emb[actual_snap].embeddings[-1] = np.zeros(snapshot_emb[actual_snap].embeddings.shape[1])\n",
    "            current_snap += 1\n",
    "        # update embeddings with sentence\n",
    "        emb.update_evolving_embeddings(s)\n",
    "    # possibly take last snapshot\n",
    "    if current_snap < len(snapshots):\n",
    "        actual_snap = dates[i]\n",
    "        temp = emb.as_pretrained()\n",
    "        snapshot_emb[actual_snap] = SimplePretrainedEmbeddings(temp.embeddings, temp.input_model)\n",
    "        snapshot_emb[actual_snap].embeddings[-1] = np.zeros(snapshot_emb[actual_snap].embeddings.shape[1])\n",
    "    print(\"Processing sentence %8i/%i...done!\" % (len(sentences), len(sentences)))\n",
    "    # reduce file size by ensuring dtype of numpy arrays is float32; transform into objects with less dependencies\n",
    "    for s in snapshot_emb:\n",
    "        snapshot_emb[s].embeddings = np.array(snapshot_emb[s].embeddings, dtype=np.float32)\n",
    "    # possibly save embeddings\n",
    "    if saveemb:\n",
    "        try:\n",
    "            pickle.dump(snapshot_emb, open(savepath, \"wb\"), -1)\n",
    "            print(\"successfully saved embeddings at %s\" % savepath)\n",
    "        except Exception as e:\n",
    "            print(\"error saving embeddings:\", e)\n",
    "    return snapshot_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_cosine_sim_knn(snapshot_emb, k=10):\n",
    "    # check the overlap between cosine similarity and knn intersection score (Gonen et al., 2020)\n",
    "    snapshots = sorted(snapshot_emb)\n",
    "    f, l = snapshots[0], snapshots[-1]\n",
    "    token_sim = []\n",
    "    token_knn_score = []\n",
    "    # ignore words that had a zero embedding in the beginning\n",
    "    tokens = [t for t in snapshot_emb[f].input_model.index2token if np.any(snapshot_emb[f][t] != 0)]\n",
    "    for i, t in enumerate(tokens):\n",
    "        if not i%100: print(\"Processing %6i/%i\" % (i+1, len(tokens)), end=\"\\r\")\n",
    "        token_sim.append(cosine_similarity(snapshot_emb[f][t][None, :], snapshot_emb[l][t][None, :])[0, 0])\n",
    "        knn1 = set(snapshot_emb[f].get_nneighbors(t, k, include_simscore=False))\n",
    "        knn2 = set(snapshot_emb[l].get_nneighbors(t, k, include_simscore=False))\n",
    "        token_knn_score.append(len(knn1.intersection(knn2))/k)\n",
    "    print(\"Processing %6i/%i\" % (len(tokens), len(tokens)))\n",
    "    token_sim, token_knn_score = np.array(token_sim), np.array(token_knn_score)\n",
    "    plt.figure()\n",
    "    plt.scatter(token_sim, token_knn_score)\n",
    "    plt.xlabel(\"cosine similarity\")\n",
    "    plt.ylabel(\"intersection of NN @ k=%i\" % k)\n",
    "    plt.title(\"correlation: %.3f\" % pearsonr(token_sim, token_knn_score)[0])\n",
    "    return tokens, token_sim, token_knn_score\n",
    "            \n",
    "            \n",
    "def most_changed_tokens(snapshot_emb, ignore_zeros=True):\n",
    "    # find the tokens whos embedding has changed the most over the whole time period\n",
    "    snapshots = sorted(snapshot_emb)\n",
    "    token_sim = []\n",
    "    for t in snapshot_emb[snapshots[0]].input_model.index2token:\n",
    "        # ignore the zero embeddings\n",
    "        if ignore_zeros:\n",
    "            token_emb = np.vstack([snapshot_emb[s][t] for s in snapshots if np.any(snapshot_emb[s][t] != 0)])\n",
    "        else:\n",
    "            token_emb = np.vstack([snapshot_emb[s][t] for s in snapshots])\n",
    "        # overall sim = min/mean of upper triangular similarity values \n",
    "        # -> take into account similarity of all emb to one another at all time points\n",
    "        if token_emb.shape[0] > 1:\n",
    "            sim = cosine_similarity(token_emb)\n",
    "            if ignore_zeros:\n",
    "                token_sim.append(sim[np.triu_indices(sim.shape[0], k=1)].min())\n",
    "            else:\n",
    "                token_sim.append(sim[np.triu_indices(sim.shape[0], k=1)].mean())\n",
    "        else:\n",
    "            token_sim.append(1)\n",
    "    # sort index from smallest to largest - the more different the word, the smaller the sim\n",
    "    token_idx = np.argsort(token_sim)\n",
    "    tokens = [(snapshot_emb[snapshots[0]].input_model.index2token[i], token_sim[i]) for i in token_idx]\n",
    "    return [t for t in tokens if t[0].isalnum()]\n",
    "\n",
    "\n",
    "def compare_most_changed_tokens(tokens1, tokens2, name1, name2, c=\"#7C0033\", new_fig=True):\n",
    "    # compare the similarity scores of the most changed tokens from two models\n",
    "    tokens1, tokens2 = dict(tokens1), dict(tokens2)\n",
    "    tokens = set(tokens1.keys())\n",
    "    tokens.intersection_update(tokens2.keys())\n",
    "    tokens = sorted(tokens)\n",
    "    scores1 = np.array([tokens1[t] for t in tokens])\n",
    "    scores2 = np.array([tokens2[t] for t in tokens])\n",
    "    if new_fig:\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        plt.grid()\n",
    "    plt.scatter(scores1, scores2, s=10, c=c, alpha=0.5)\n",
    "    plt.xlabel(name1, fontsize=14)\n",
    "    plt.ylabel(name2, fontsize=14)\n",
    "    corr = pearsonr(scores1, scores2)[0]\n",
    "    plt.title(\"correlation: %.3f\" % corr)\n",
    "    return corr\n",
    "    \n",
    "\n",
    "def analyze_emb_over_time(snapshot_emb, token, k=5, savefigs=\"\"):\n",
    "    snapshots = sorted(snapshot_emb)\n",
    "    # get the two snapshots where the embeddings of the token are the most different\n",
    "    snapshots_nonz = [s for s in snapshots if np.any(snapshot_emb[s][token] != 0)]\n",
    "    if len(snapshots_nonz) > 1:\n",
    "        token_emb = np.vstack([snapshot_emb[s][token] for s in snapshots_nonz])\n",
    "        sim = cosine_similarity(token_emb)\n",
    "        rowidx, colidx = np.triu_indices(sim.shape[0], k=1)\n",
    "        minidx = sim[rowidx, colidx].argmin()\n",
    "        first, last = snapshots_nonz[rowidx[minidx]], snapshots_nonz[colidx[minidx]]\n",
    "    else:\n",
    "        first, last = snapshots[0], snapshots_nonz[0]\n",
    "    \n",
    "    # get the corresponding nearest neighbors\n",
    "    nn_first = snapshot_emb[first].get_nneighbors(token, k, include_simscore=False)\n",
    "    nn_last = snapshot_emb[last].get_nneighbors(token, k, include_simscore=False)\n",
    "    \n",
    "    # get colors for plots later\n",
    "    colors = {}\n",
    "    colors[token] = (0., 0., 0., 1.)\n",
    "    colors[\"%s (%s)\" % (token, first)] = (0., 0., 0., 1.)\n",
    "    colors[\"%s (%s)\" % (token, last)] = (0., 0., 0., 1.)\n",
    "    cmap = plt.get_cmap(\"RdBu\")\n",
    "    for i, t in enumerate(nn_first):\n",
    "        colors[t] = cmap(0.4*(i/(k-1)))\n",
    "    for i, t in enumerate(nn_last):\n",
    "        if t not in colors:\n",
    "            colors[t] = cmap(1-0.4*(i/(k-1)))\n",
    "    # plotly colors (careful - wants css colors)\n",
    "    color_plotly = {t: \"rgb(%i,%i,%i)\" % tuple(k*255 for k in v[:3]) for t, v in colors.items()}\n",
    "    # make sure nn_last only contains tokens not in nn_first\n",
    "    nn_last = [t for t in nn_last if t not in nn_first]\n",
    "    \n",
    "    # create embedding matrices per token over time\n",
    "    token_emb = {}\n",
    "    for t in [token] + nn_first + nn_last:\n",
    "        token_emb[t] = np.vstack([snapshot_emb[s][t] for s in snapshots])\n",
    "        \n",
    "    # compute similarity of each nn to the token\n",
    "    sim_scores = {}\n",
    "    for t in nn_first + nn_last:\n",
    "        sim_scores[t] = np.diag(cosine_similarity(token_emb[token], token_emb[t]))\n",
    "    # similarity of token itself to beginning and end embedding\n",
    "    sim_scores[\"%s (%s)\" % (token, first)] = cosine_similarity(token_emb[token], token_emb[token][[0]]).flatten()\n",
    "    sim_scores[\"%s (%s)\" % (token, last)] = cosine_similarity(token_emb[token], token_emb[token][[-1]]).flatten()\n",
    "                \n",
    "    # plot evolution of similarity scores over time\n",
    "    snapshot_dates = list(range(len(snapshots)))\n",
    "    plot_tokens = [\"%s (%s)\" % (token, first), \"%s (%s)\" % (token, last)] + nn_first + nn_last\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    for t in plot_tokens:\n",
    "        plt.plot(snapshot_dates, sim_scores[t], \"--\" if t == \"%s (%s)\" % (token, first) else \"-\", color=colors[t], label=t)\n",
    "    l = plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0., fontsize=14)\n",
    "    plt.xticks(snapshot_dates, snapshots, rotation=90 if len(snapshots) > 5 else 0)\n",
    "    plt.ylabel(\"cosine similarity\")\n",
    "    # plt.title(token)\n",
    "    if savefigs:\n",
    "        plt.savefig(\"%s_%s_%s_%s_time.pdf\" % (savefigs, token, snapshots[0], snapshots[-1]), dpi=300, bbox_inches=\"tight\", bbox_extra_artists=[l])\n",
    "    # interactive timelines with plotly\n",
    "    df_temp = pd.DataFrame({\n",
    "        \"snapshot date\": [datetime.strptime(s, '%Y-%m-%d') for t in plot_tokens for s in snapshots],\n",
    "        \"cosine similarity\": np.array([sim_scores[t] for t in plot_tokens]).flatten(),\n",
    "        \"token\": [t for t in plot_tokens for s in snapshots],\n",
    "        \"line\": [\"dash\" if t == \"%s (%s)\" % (token, first) else \"solid\" for t in plot_tokens for s in snapshots]\n",
    "    })\n",
    "    fig_time = px.line(df_temp, x=\"snapshot date\", y=\"cosine similarity\", \n",
    "                       color=\"token\", color_discrete_map=color_plotly, hover_name=\"token\", \n",
    "                       line_dash=\"line\", line_dash_map='identity')\n",
    "    fig_time.show()\n",
    "\n",
    "    # plot 2D PCA vis of embeddings\n",
    "    full_embedding_mat = []\n",
    "    labels = []\n",
    "    color_keys = []\n",
    "    size = []\n",
    "    for t in [token] + nn_first + nn_last:\n",
    "        full_embedding_mat.append(token_emb[t])\n",
    "        labels.extend([\"%s (%s)\" % (t, s) for s in snapshots])\n",
    "        color_keys.extend(len(snapshots)*[t])\n",
    "        size.extend(list(range(1, len(snapshots) + 1)))\n",
    "    full_embedding_mat = np.vstack(full_embedding_mat)\n",
    "    X_kpca = KernelPCA(n_components=2, kernel=\"cosine\").fit_transform(full_embedding_mat)\n",
    "    # with matplotlib\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.scatter(x=X_kpca[:, 0], y=X_kpca[:, 1], s=10*np.array(size), c=[colors[t] for t in color_keys], alpha=0.6)\n",
    "    plt.xticks([], [])\n",
    "    plt.yticks([], [])\n",
    "    plt.xlabel(\"PC 1\")\n",
    "    plt.ylabel(\"PC 2\")\n",
    "    # plt.title(token)\n",
    "    if savefigs:\n",
    "        plt.savefig(\"%s_%s_%s_%s_pca.pdf\" % (savefigs, token, snapshots[0], snapshots[-1]), dpi=300, bbox_inches=\"tight\")\n",
    "    # interactive with plotly\n",
    "    fig_pca = px.scatter(x=X_kpca[:, 0], y=X_kpca[:, 1], color=color_keys, size=np.sqrt(size), color_discrete_map=color_plotly, hover_name=labels)\n",
    "    fig_pca.update_traces(hovertemplate='%{hovertext}')  # only show our text, no additional info\n",
    "    fig_pca.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre and post-corona outbreak in detail\n",
    "snapshots = [\"2019-%02i-31\" % i for i in range(6, 13)] + [\"2020-%02i-31\" % i for i in range(1, 13)]\n",
    "\n",
    "def run_analysis(local_emb_name=\"dummy\", savefigs=\"\", check_knn_score=False):   \n",
    "    # generate/load embeddings\n",
    "    snapshot_emb = get_emb_snapshots(snapshots, start_date=\"2019-04-01\", local_emb_name=local_emb_name, min_freq=50, n_tokens=10000, saveemb=True)\n",
    "    # see which words have changed the most at some point in the time period\n",
    "    most_changed = most_changed_tokens(snapshot_emb, ignore_zeros=True)\n",
    "    print(\"most changed tokens (ignore_zeros=True)\")\n",
    "    print(\"\\n\".join([\"%15s (%.4f)\" % x for x in most_changed[:25]]))\n",
    "    # see which words are new\n",
    "    print(\"most changed tokens (ignore_zeros=False)\")\n",
    "    print(\"\\n\".join([\"%15s (%.4f)\" % x for x in most_changed_tokens(snapshot_emb, ignore_zeros=False)[:25]]))\n",
    "    if check_knn_score:\n",
    "        # see in how far the cosine similarity and knn intersection score agree\n",
    "        for k in [10, 100, 1000]:\n",
    "            tokens, token_sim, token_knn_score = test_cosine_sim_knn(snapshot_emb, k=k)\n",
    "    # create plots from paper\n",
    "    analyze_emb_over_time(snapshot_emb, \"positive\", k=5, savefigs=savefigs)\n",
    "    analyze_emb_over_time(snapshot_emb, \"category\", k=5, savefigs=savefigs)\n",
    "    return snapshot_emb, most_changed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# run analysis for bert\n",
    "snapshot_emb, bert_most_changed = run_analysis(local_emb_name=\"bert\", savefigs=\"bert\")\n",
    "analyze_emb_over_time(snapshot_emb, \"biden\", k=5, savefigs=\"bert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# same analysis for roberta\n",
    "_, roberta_most_changed = run_analysis(local_emb_name=\"roberta\")\n",
    "# and both finetuned models\n",
    "snapshot_emb, bert_ft_most_changed = run_analysis(local_emb_name=\"nyt_bert\")\n",
    "analyze_emb_over_time(snapshot_emb, \"biden\", k=5, savefigs=\"nyt_bert\")\n",
    "_, roberta_ft_most_changed = run_analysis(local_emb_name=\"nyt_roberta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see in how far the most changed tokens from BERT and RoBERTa agree (before and after fine-tuning)\n",
    "_ = compare_most_changed_tokens(bert_most_changed, bert_ft_most_changed, \"BERT\", \"BERT (fine-tuned)\")\n",
    "_ = compare_most_changed_tokens(roberta_most_changed, roberta_ft_most_changed, \"RoBERTa\", \"RoBERTa (fine-tuned)\")\n",
    "corr1 = compare_most_changed_tokens(bert_most_changed, roberta_most_changed, \"BERT\", \"RoBERTa\")\n",
    "corr_ft = compare_most_changed_tokens(bert_ft_most_changed, roberta_ft_most_changed, \"BERT\", \"RoBERTa\", c=\"#00537C\", new_fig=False)\n",
    "plt.title(\"\")\n",
    "plt.legend([\"pre-trained $(r: %.3f)$\" % corr1, \"fine-tuned $(r: %.3f)$\" % corr_ft], fontsize=14)\n",
    "plt.savefig(\"diachr_score_agreement.pdf\", dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
